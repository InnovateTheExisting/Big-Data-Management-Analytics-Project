# Read all JSON files generated by crawler in a directory and add them to mongo collection
from pymongo import MongoClient
import glob
import json

client = MongoClient()

db = client['big_data']
articles = db['spanish_articles']

source_folder = "../Crawler/data/data/2019/05"

if __name__ == "__main__":
    # read only json files and add relevant into to database
    for file in glob.glob(source_folder + '/*.json'):
        with open(file, 'r') as curr_file:
            data = json.load(curr_file)
            if data.get('language', '') == 'es':
                articles.insert({
                    'authors': data.get('authors', []),
                    'date': data.get('date_publish', ''),
                    'description': data.get('description', ''),
                    'title': data.get('title', ''),
                    'text': data.get('text', ''),
                    'url': data.get('url', '')
                })
